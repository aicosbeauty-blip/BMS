#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OECDç½‘ç«™å®Œæ•´è§£å†³æ–¹æ¡ˆ - å¸¦JSONå¯¼å‡º
è‡ªåŠ¨æå–æ•°æ®ã€ä¸‹è½½PDFï¼Œå¹¶ç”ŸæˆJSONæŠ¥å‘Š
"""

import os
import time
import json
import requests
from pathlib import Path
from urllib.parse import urljoin
from datetime import datetime

try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.chrome.service import Service
    from webdriver_manager.chrome import ChromeDriverManager
    from bs4 import BeautifulSoup
    DEPS_AVAILABLE = True
except ImportError:
    DEPS_AVAILABLE = False
    print("âš  éœ€è¦å®‰è£…: pip install selenium webdriver-manager beautifulsoup4")


def find_all_pdf_links(html_content, base_url):
    """
    ä»HTMLä¸­æŸ¥æ‰¾æ‰€æœ‰PDFé“¾æ¥ï¼ŒåŒ…æ‹¬handler.axdæ ¼å¼
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    pdf_links = []
    
    # æŸ¥æ‰¾æ‰€æœ‰<a>æ ‡ç­¾
    for link in soup.find_all('a', href=True):
        href = link['href']
        text = link.get_text(strip=True)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºPDFé“¾æ¥ï¼ˆ.pdfç»“å°¾æˆ–åŒ…å«handler.axdï¼‰
        if '.pdf' in href.lower() or 'handler.axd' in href.lower():
            full_url = urljoin(base_url, href)
            
            # æå–æ–‡ä»¶å
            if text and text.endswith('.pdf'):
                filename = text
            elif '.pdf' in href.lower():
                filename = href.split('/')[-1]
                if '?' in filename:
                    filename = filename.split('?')[0]
            else:
                filename = text if text else 'document.pdf'
                if not filename.endswith('.pdf'):
                    filename += '.pdf'
            
            pdf_links.append({
                'filename': filename,
                'url': full_url,
                'source': 'link',
                'link_text': text
            })
    
    # æŸ¥æ‰¾embedã€objectã€iframeæ ‡ç­¾
    for tag in ['embed', 'object', 'iframe']:
        for elem in soup.find_all(tag):
            src = elem.get('src') or elem.get('data')
            if src and ('.pdf' in src.lower() or 'handler.axd' in src.lower()):
                full_url = urljoin(base_url, src)
                filename = src.split('/')[-1] or 'document.pdf'
                if not filename.endswith('.pdf'):
                    filename += '.pdf'
                
                pdf_links.append({
                    'filename': filename,
                    'url': full_url,
                    'source': tag,
                    'link_text': ''
                })
    
    # å»é‡
    seen_urls = set()
    unique_links = []
    for link in pdf_links:
        if link['url'] not in seen_urls:
            seen_urls.add(link['url'])
            unique_links.append(link)
    
    return unique_links


def extract_chemical_data(html_content):
    """ä»SIDS.aspx iframeæå–åŒ–å­¦å“æ•°æ®"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    data = {}
    
    # æå–å…³é”®å­—æ®µ
    fields = {
        'CasnumLabel': 'cas_number',
        'SYNONYMLLabel': 'chemical_name',
        'OtherSynonymsLabel': 'synonyms',
        'InHPVLabel': 'hpv_status',
        'RecoLowHazardLabel': 'recognized_low_hazard',
        'IndInitiativeLabel': 'on_icca_list',
        'AddremarksLabel': 'additional_information'
    }
    
    for span_id, field_name in fields.items():
        elem = soup.find('span', id=span_id)
        data[field_name] = elem.text.strip() if elem and elem.text.strip() else None
    
    return data


def extract_assessment_data(html_content):
    """ä»SidsOrganigrame.aspx iframeæå–è¯„ä¼°æ•°æ®"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    data = {}
    
    # æå–å…³é”®å­—æ®µ
    fields = {
        'SponsorsLabel': 'sponsors',
        'SponsorshipDateLabel': 'sponsorship_date',
        'CurrentStatusLabel': 'current_status',
        'MeetingSIAMLabel': 'assessment_meeting',
        'DatePublishedLabel': 'date_published',
        'TargetedAssessmentLabel': 'targeted_assessment',
    }
    
    for span_id, field_name in fields.items():
        elem = soup.find('span', id=span_id)
        data[field_name] = elem.text.strip() if elem and elem.text.strip() else None
    
    # æå–Category
    category_link = soup.find('a', id='CategoryHL')
    if category_link:
        data['category'] = category_link.text.strip()
        data['category_link'] = category_link.get('href', '')
    else:
        data['category'] = None
        data['category_link'] = None
    
    # æå–ICCAå¤‡æ³¨
    icca_label = soup.find('span', id='SidsOrganigrame_ICCA_Label')
    if icca_label:
        data['icca_note'] = icca_label.text.strip()
    else:
        data['icca_note'] = None
    
    return data


def download_pdf(url, output_path, session):
    """
    ä¸‹è½½PDFæ–‡ä»¶
    
    è¿”å›: (success, file_size, error_message)
    """
    try:
        response = session.get(url, timeout=60, stream=True)
        response.raise_for_status()
        
        with open(output_path, 'wb') as f:
            for chunk in response.iter_content(8192):
                if chunk:
                    f.write(chunk)
        
        file_size = os.path.getsize(output_path)
        return True, file_size, None
        
    except Exception as e:
        return False, 0, str(e)


def test_oecd_with_json(url, output_folder="oecd_json_test"):
    """
    å®Œæ•´çš„OECDç½‘ç«™æµ‹è¯•ï¼Œç”ŸæˆJSONæŠ¥å‘Š
    """
    if not DEPS_AVAILABLE:
        print("\nâŒ ç¼ºå°‘å¿…è¦çš„åº“")
        print("è¯·è¿è¡Œ: pip install selenium webdriver-manager beautifulsoup4 requests")
        return None
    
    print("=" * 80)
    print("OECDå®Œæ•´æµ‹è¯• - å¸¦JSONå¯¼å‡º")
    print("=" * 80)
    print(f"URL: {url}\n")
    
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    Path(output_folder).mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–ç»“æœæ•°æ®ç»“æ„
    result = {
        'metadata': {
            'source_url': url,
            'extraction_time': datetime.now().isoformat(),
            'version': '1.0'
        },
        'chemical_info': {},
        'assessment_info': {},
        'pdf_files': [],
        'iframes': []
    }
    
    driver = None
    all_pdf_links = []
    
    try:
        # 1. åˆå§‹åŒ–Selenium
        print("æ­¥éª¤1: åˆå§‹åŒ–æµè§ˆå™¨...")
        service = Service(ChromeDriverManager().install())
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--window-size=1920,1080')
        
        driver = webdriver.Chrome(service=service, options=options)
        print("âœ“ æµè§ˆå™¨å·²å¯åŠ¨\n")
        
        # 2. è®¿é—®ä¸»é¡µé¢
        print("æ­¥éª¤2: è®¿é—®ä¸»é¡µé¢...")
        driver.get(url)
        time.sleep(3)
        
        main_html = driver.page_source
        print(f"âœ“ ä¸»é¡µé¢å¤§å°: {len(main_html)} å­—ç¬¦\n")
        
        # 3. æŸ¥æ‰¾æ‰€æœ‰iframe
        print("æ­¥éª¤3: æŸ¥æ‰¾å¹¶è®¿é—®iframe...")
        iframes = driver.find_elements(By.TAG_NAME, "iframe")
        print(f"æ‰¾åˆ° {len(iframes)} ä¸ªiframe\n")
        
        # 4. è®¿é—®æ¯ä¸ªiframe
        for i, iframe in enumerate(iframes, 1):
            try:
                iframe_id = iframe.get_attribute('id') or f"iframe_{i}"
                iframe_src = iframe.get_attribute('src')
                
                if not iframe_src or 'about:blank' in iframe_src:
                    continue
                
                print(f"[iframe {i}] {iframe_id}")
                print(f"  SRC: {iframe_src}")
                
                # åˆ‡æ¢åˆ°iframe
                driver.switch_to.default_content()
                driver.switch_to.frame(iframe)
                time.sleep(2)
                
                # è·å–å†…å®¹
                iframe_html = driver.page_source
                iframe_base_url = urljoin(url, iframe_src)
                
                # ä¿å­˜iframe HTML
                filename = f"iframe_{i}_{iframe_id.replace(':', '_')}.html"
                filepath = os.path.join(output_folder, filename)
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(iframe_html)
                
                print(f"  âœ“ å·²ä¿å­˜: {filename} ({len(iframe_html)} å­—ç¬¦)")
                
                # è®°å½•iframeä¿¡æ¯
                iframe_info = {
                    'index': i,
                    'id': iframe_id,
                    'src': iframe_src,
                    'html_file': filename,
                    'html_size': len(iframe_html)
                }
                
                # æŸ¥æ‰¾PDFé“¾æ¥
                pdfs = find_all_pdf_links(iframe_html, iframe_base_url)
                if pdfs:
                    print(f"  âœ“ æ‰¾åˆ° {len(pdfs)} ä¸ªPDF:")
                    for pdf in pdfs:
                        print(f"    - {pdf['filename']}")
                    all_pdf_links.extend(pdfs)
                    iframe_info['pdf_count'] = len(pdfs)
                else:
                    iframe_info['pdf_count'] = 0
                
                # æå–æ•°æ®
                if 'SIDS.aspx' in iframe_src:
                    result['chemical_info'] = extract_chemical_data(iframe_html)
                    iframe_info['type'] = 'chemical_info'
                    print(f"  âœ“ æå–äº†åŒ–å­¦å“æ•°æ®")
                elif 'SidsOrganigrame.aspx' in iframe_src:
                    result['assessment_info'] = extract_assessment_data(iframe_html)
                    iframe_info['type'] = 'assessment_info'
                    print(f"  âœ“ æå–äº†è¯„ä¼°æ•°æ®")
                else:
                    iframe_info['type'] = 'other'
                
                result['iframes'].append(iframe_info)
                print()
                
                # åˆ‡æ¢å›ä¸»é¡µé¢
                driver.switch_to.default_content()
                
            except Exception as e:
                print(f"  âœ— å¤„ç†å¤±è´¥: {e}\n")
                driver.switch_to.default_content()
                continue
        
        # 5. ç›´æ¥è®¿é—®iframe URL
        print("æ­¥éª¤4: ç›´æ¥è®¿é—®iframe URL...")
        
        soup = BeautifulSoup(main_html, 'html.parser')
        direct_urls = []
        for iframe_elem in soup.find_all('iframe'):
            src = iframe_elem.get('src')
            if src and 'about:blank' not in src:
                full_url = urljoin(url, src)
                direct_urls.append(full_url)
        
        for i, direct_url in enumerate(direct_urls, 1):
            try:
                print(f"\n[ç›´æ¥è®¿é—® {i}] {direct_url}")
                driver.get(direct_url)
                time.sleep(2)
                
                direct_html = driver.page_source
                
                # ä¿å­˜HTML
                filename = f"direct_iframe_{i}.html"
                filepath = os.path.join(output_folder, filename)
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(direct_html)
                
                print(f"  âœ“ å·²ä¿å­˜: {filename} ({len(direct_html)} å­—ç¬¦)")
                
                # æŸ¥æ‰¾PDF
                pdfs = find_all_pdf_links(direct_html, direct_url)
                if pdfs:
                    print(f"  âœ“ æ‰¾åˆ° {len(pdfs)} ä¸ªPDF:")
                    for pdf in pdfs:
                        print(f"    - {pdf['filename']}")
                    all_pdf_links.extend(pdfs)
                
                # æå–æ•°æ®ï¼ˆè¡¥å……æˆ–æ›´æ–°ï¼‰
                if 'SIDS.aspx' in direct_url:
                    chem_data = extract_chemical_data(direct_html)
                    if chem_data:
                        result['chemical_info'].update(chem_data)
                elif 'SidsOrganigrame.aspx' in direct_url:
                    assess_data = extract_assessment_data(direct_html)
                    if assess_data:
                        result['assessment_info'].update(assess_data)
                
            except Exception as e:
                print(f"  âœ— è®¿é—®å¤±è´¥: {e}")
                continue
        
        # 6. å»é‡PDFé“¾æ¥
        seen = set()
        unique_pdfs = []
        for pdf in all_pdf_links:
            if pdf['url'] not in seen:
                seen.add(pdf['url'])
                unique_pdfs.append(pdf)
        
        all_pdf_links = unique_pdfs
        
        # 7. æ˜¾ç¤ºæå–çš„æ•°æ®
        print("\n" + "=" * 80)
        print("æå–çš„åŒ–å­¦å“æ•°æ®")
        print("=" * 80)
        for key, value in result['chemical_info'].items():
            print(f"{key:.<35} {value}")
        
        print("\n" + "=" * 80)
        print("æå–çš„è¯„ä¼°æ•°æ®")
        print("=" * 80)
        for key, value in result['assessment_info'].items():
            print(f"{key:.<35} {value}")
        
        # 8. æ˜¾ç¤ºPDFé“¾æ¥
        print("\n" + "=" * 80)
        print(f"æ‰¾åˆ° {len(all_pdf_links)} ä¸ªPDFæ–‡ä»¶")
        print("=" * 80)
        
        if not all_pdf_links:
            print("\nâš  æœªæ‰¾åˆ°PDFé“¾æ¥")
        else:
            # å‡†å¤‡ä¸‹è½½
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            
            # ä¸‹è½½æ¯ä¸ªPDF
            print("\næ­¥éª¤5: ä¸‹è½½PDFæ–‡ä»¶...\n")
            
            for i, pdf in enumerate(all_pdf_links, 1):
                print(f"[PDF {i}/{len(all_pdf_links)}]")
                print(f"  æ–‡ä»¶å: {pdf['filename']}")
                print(f"  URL: {pdf['url']}")
                
                # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
                safe_filename = pdf['filename'].replace('/', '_').replace('\\', '_')
                local_path = os.path.join(output_folder, safe_filename)
                
                print(f"  æ­£åœ¨ä¸‹è½½...")
                success, file_size, error = download_pdf(pdf['url'], local_path, session)
                
                # å‡†å¤‡PDFä¿¡æ¯
                pdf_info = {
                    'index': i,
                    'filename': safe_filename,
                    'original_filename': pdf['filename'],
                    'url': pdf['url'],
                    'link_text': pdf.get('link_text', ''),
                    'source': pdf.get('source', ''),
                    'local_path': os.path.abspath(local_path),
                    'download_success': success
                }
                
                if success:
                    pdf_info['file_size_bytes'] = file_size
                    pdf_info['file_size_kb'] = round(file_size / 1024, 2)
                    pdf_info['file_size_mb'] = round(file_size / (1024 * 1024), 2)
                    print(f"  âœ“ æˆåŠŸ!")
                    print(f"     å¤§å°: {file_size:,} å­—èŠ‚ ({pdf_info['file_size_kb']:.2f} KB)")
                    print(f"     è·¯å¾„: {pdf_info['local_path']}")
                else:
                    pdf_info['error'] = error
                    print(f"  âœ— å¤±è´¥: {error}")
                
                result['pdf_files'].append(pdf_info)
                print()
                
                time.sleep(1)
            
            # ç»Ÿè®¡
            success_count = sum(1 for p in result['pdf_files'] if p['download_success'])
            print(f"ä¸‹è½½å®Œæˆ: {success_count}/{len(all_pdf_links)} ä¸ªPDF")
        
        # 9. ä¿å­˜JSON
        print("\n" + "=" * 80)
        print("ä¿å­˜JSONæŠ¥å‘Š")
        print("=" * 80)
        
        json_path = os.path.join(output_folder, "oecd_data.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ JSONå·²ä¿å­˜: {os.path.abspath(json_path)}")
        
        # ä¹Ÿä¿å­˜ä¸€ä¸ªæ ¼å¼åŒ–çš„æ˜“è¯»ç‰ˆæœ¬
        pretty_json_path = os.path.join(output_folder, "oecd_data_pretty.json")
        with open(pretty_json_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=4, sort_keys=True)
        
        print(f"âœ“ æ ¼å¼åŒ–JSONå·²ä¿å­˜: {os.path.abspath(pretty_json_path)}")
        
        # 10. æœ€ç»ˆæ€»ç»“
        print("\n" + "=" * 80)
        print("æµ‹è¯•å®Œæˆï¼")
        print("=" * 80)
        print(f"\nğŸ“ æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨: {os.path.abspath(output_folder)}\n")
        
        # ç»Ÿè®¡ä¿¡æ¯
        print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  - åŒ–å­¦å“å­—æ®µ: {len(result['chemical_info'])} ä¸ª")
        print(f"  - è¯„ä¼°å­—æ®µ: {len(result['assessment_info'])} ä¸ª")
        print(f"  - å¤„ç†çš„iframe: {len(result['iframes'])} ä¸ª")
        print(f"  - PDFæ–‡ä»¶: {len(result['pdf_files'])} ä¸ª")
        if result['pdf_files']:
            success_pdfs = [p for p in result['pdf_files'] if p['download_success']]
            print(f"  - æˆåŠŸä¸‹è½½: {len(success_pdfs)} ä¸ª")
            total_size = sum(p.get('file_size_kb', 0) for p in success_pdfs)
            print(f"  - æ€»å¤§å°: {total_size:.2f} KB")
        
        # åˆ—å‡ºæ–‡ä»¶
        files = os.listdir(output_folder)
        html_files = [f for f in files if f.endswith('.html')]
        pdf_files = [f for f in files if f.endswith('.pdf')]
        json_files = [f for f in files if f.endswith('.json')]
        
        print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        if json_files:
            print(f"  JSONæ–‡ä»¶ ({len(json_files)}ä¸ª):")
            for f in sorted(json_files):
                size = os.path.getsize(os.path.join(output_folder, f))
                print(f"    - {f} ({size:,} å­—èŠ‚)")
        
        if pdf_files:
            print(f"  PDFæ–‡ä»¶ ({len(pdf_files)}ä¸ª):")
            for f in sorted(pdf_files):
                size = os.path.getsize(os.path.join(output_folder, f))
                print(f"    - {f} ({size:,} å­—èŠ‚)")
        
        if html_files:
            print(f"  HTMLæ–‡ä»¶ ({len(html_files)}ä¸ª): {len(html_files)}ä¸ªiframeå†…å®¹")
        
        print("\n" + "=" * 80)
        
        return result
        
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None
    
    finally:
        if driver:
            driver.quit()
            print("\næµè§ˆå™¨å·²å…³é—­")


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    print("\n" + "=" * 80)
    print("OECDå®Œæ•´æµ‹è¯• - å¸¦JSONå¯¼å‡º")
    print("=" * 80)
    print("\nåŠŸèƒ½:")
    print("  âœ“ è‡ªåŠ¨å¤„ç†iframeç»“æ„")
    print("  âœ“ è¯†åˆ«handler.axdæ ¼å¼PDFé“¾æ¥")
    print("  âœ“ æå–å®Œæ•´åŒ–å­¦å“å’Œè¯„ä¼°æ•°æ®")
    print("  âœ“ ä¸‹è½½æ‰€æœ‰PDFæ–‡ä»¶ï¼ˆæ”¯æŒå¤šä¸ªï¼‰")
    print("  âœ“ ç”ŸæˆJSONæŠ¥å‘Šï¼ˆåŒ…å«æ‰€æœ‰æ•°æ®å’ŒPDFä¿¡æ¯ï¼‰")
    print("\n" + "=" * 80 + "\n")
    
    # é»˜è®¤URL
    test_url = "https://hpvchemicals.oecd.org/ui/SIDS_Details.aspx?id=b1b28e5c-118a-4d76-ad61-e9fe4cb9aa30"
    
    # å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        test_url = sys.argv[1]
    
    result = test_oecd_with_json(test_url)
    
    if result:
        print("\nâœ… JSONæ•°æ®ç»“æ„:")
        print(f"  - metadata: å…ƒæ•°æ®ï¼ˆURLã€æ—¶é—´ç­‰ï¼‰")
        print(f"  - chemical_info: åŒ–å­¦å“åŸºæœ¬ä¿¡æ¯")
        print(f"  - assessment_info: èµåŠ©å’Œè¯„ä¼°ä¿¡æ¯")
        print(f"  - pdf_files: PDFæ–‡ä»¶åˆ—è¡¨ï¼ˆ{len(result['pdf_files'])}ä¸ªï¼‰")
        print(f"  - iframes: iframeå¤„ç†ä¿¡æ¯")
        print("\nå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹JSON:")
        print("  cat oecd_json_test/oecd_data.json")
        print("  cat oecd_json_test/oecd_data_pretty.json")


if __name__ == "__main__":
    main()