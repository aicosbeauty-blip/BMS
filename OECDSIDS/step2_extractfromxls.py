#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OECDæ‰¹é‡å¤„ç† - Excelç‰ˆæœ¬
ä»Excelè¯»å–URLï¼Œæ‰¹é‡æå–æ•°æ®å’Œä¸‹è½½PDFï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
"""

import os
import time
import json
import hashlib
import requests
import pandas as pd
from pathlib import Path
from urllib.parse import urljoin
from datetime import datetime

try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.chrome.service import Service
    from webdriver_manager.chrome import ChromeDriverManager
    from bs4 import BeautifulSoup
    DEPS_AVAILABLE = True
except ImportError:
    DEPS_AVAILABLE = False
    print("âš  éœ€è¦å®‰è£…: pip install selenium webdriver-manager beautifulsoup4 pandas openpyxl")


def calculate_file_md5(filepath):
    """
    è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼
    
    å‚æ•°:
        filepath: æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        MD5å“ˆå¸Œå€¼ï¼ˆ32ä½å°å†™åå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼‰
    """
    md5_hash = hashlib.md5()
    
    with open(filepath, "rb") as f:
        # åˆ†å—è¯»å–ï¼Œé€‚åˆå¤§æ–‡ä»¶
        for chunk in iter(lambda: f.read(4096), b""):
            md5_hash.update(chunk)
    
    return md5_hash.hexdigest()


def find_all_pdf_links(html_content, base_url):
    """ä»HTMLä¸­æŸ¥æ‰¾æ‰€æœ‰PDFé“¾æ¥ï¼ŒåŒ…æ‹¬handler.axdæ ¼å¼"""
    soup = BeautifulSoup(html_content, 'html.parser')
    pdf_links = []
    
    # æŸ¥æ‰¾æ‰€æœ‰<a>æ ‡ç­¾
    for link in soup.find_all('a', href=True):
        href = link['href']
        text = link.get_text(strip=True)
        
        if '.pdf' in href.lower() or 'handler.axd' in href.lower():
            full_url = urljoin(base_url, href)
            
            # æå–æ–‡ä»¶å
            if text and text.endswith('.pdf'):
                filename = text
            elif '.pdf' in href.lower():
                filename = href.split('/')[-1]
                if '?' in filename:
                    filename = filename.split('?')[0]
            else:
                filename = text if text else 'document.pdf'
                if not filename.endswith('.pdf'):
                    filename += '.pdf'
            
            pdf_links.append({
                'filename': filename,
                'url': full_url,
                'source': 'link'
            })
    
    # æŸ¥æ‰¾embedã€objectã€iframeæ ‡ç­¾
    for tag in ['embed', 'object', 'iframe']:
        for elem in soup.find_all(tag):
            src = elem.get('src') or elem.get('data')
            if src and ('.pdf' in src.lower() or 'handler.axd' in src.lower()):
                full_url = urljoin(base_url, src)
                filename = src.split('/')[-1] or 'document.pdf'
                if not filename.endswith('.pdf'):
                    filename += '.pdf'
                
                pdf_links.append({
                    'filename': filename,
                    'url': full_url,
                    'source': tag
                })
    
    # å»é‡
    seen_urls = set()
    unique_links = []
    for link in pdf_links:
        if link['url'] not in seen_urls:
            seen_urls.add(link['url'])
            unique_links.append(link)
    
    return unique_links


def extract_chemical_data(html_content):
    """ä»SIDS.aspx iframeæå–åŒ–å­¦å“æ•°æ®"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    data = {}
    
    fields = {
        'CasnumLabel': 'cas_number',
        'SYNONYMLLabel': 'chemical_name',
        'OtherSynonymsLabel': 'synonyms',
        'InHPVLabel': 'hpv_status',
        'RecoLowHazardLabel': 'recognized_low_hazard',
        'IndInitiativeLabel': 'on_icca_list',
        'AddremarksLabel': 'additional_information'
    }
    
    for span_id, field_name in fields.items():
        elem = soup.find('span', id=span_id)
        data[field_name] = elem.text.strip() if elem and elem.text.strip() else None
    
    return data


def extract_assessment_data(html_content):
    """ä»SidsOrganigrame.aspx iframeæå–è¯„ä¼°æ•°æ®"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    data = {}
    
    fields = {
        'SponsorsLabel': 'sponsors',
        'SponsorshipDateLabel': 'sponsorship_date',
        'CurrentStatusLabel': 'current_status',
        'MeetingSIAMLabel': 'assessment_meeting',
        'DatePublishedLabel': 'date_published',
        'TargetedAssessmentLabel': 'targeted_assessment',
    }
    
    for span_id, field_name in fields.items():
        elem = soup.find('span', id=span_id)
        data[field_name] = elem.text.strip() if elem and elem.text.strip() else None
    
    # æå–Category
    category_link = soup.find('a', id='CategoryHL')
    if category_link:
        data['category'] = category_link.text.strip()
        data['category_link'] = category_link.get('href', '')
    else:
        data['category'] = None
        data['category_link'] = None
    
    # æå–ICCAå¤‡æ³¨
    icca_label = soup.find('span', id='SidsOrganigrame_ICCA_Label')
    if icca_label:
        data['icca_note'] = icca_label.text.strip()
    else:
        data['icca_note'] = None
    
    return data


def download_pdf_with_md5(url, output_folder, session):
    """
    ä¸‹è½½PDFæ–‡ä»¶å¹¶ç”¨MD5å‘½å
    
    è¿”å›: (success, original_filename, md5_filename, file_size, error_message)
    """
    try:
        response = session.get(url, timeout=60, stream=True)
        response.raise_for_status()
        
        # å…ˆä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶
        temp_path = os.path.join(output_folder, f"temp_{int(time.time())}.pdf")
        
        with open(temp_path, 'wb') as f:
            for chunk in response.iter_content(8192):
                if chunk:
                    f.write(chunk)
        
        # è®¡ç®—MD5
        md5_hash = calculate_file_md5(temp_path)
        
        # é‡å‘½åä¸ºMD5
        md5_filename = f"{md5_hash}.pdf"
        final_path = os.path.join(output_folder, md5_filename)
        
        # å¦‚æœMD5æ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ é™¤ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(final_path):
            os.remove(temp_path)
            file_size = os.path.getsize(final_path)
            return True, None, md5_filename, file_size, None
        else:
            # é‡å‘½åä¸´æ—¶æ–‡ä»¶
            os.rename(temp_path, final_path)
            file_size = os.path.getsize(final_path)
            return True, None, md5_filename, file_size, None
        
    except Exception as e:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if 'temp_path' in locals() and os.path.exists(temp_path):
            try:
                os.remove(temp_path)
            except:
                pass
        return False, None, None, 0, str(e)


def process_single_url(url, pdf_folder, driver, session):
    """
    å¤„ç†å•ä¸ªURLï¼Œæå–æ•°æ®å’Œä¸‹è½½PDF
    
    å‚æ•°:
        url: OECD URL
        pdf_folder: PDFä¿å­˜æ–‡ä»¶å¤¹
        driver: Selenium WebDriver
        session: requests Session
    
    è¿”å›:
        result dict æˆ– Noneï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    result = {
        'metadata': {
            'source_url': url,
            'extraction_time': datetime.now().isoformat(),
            'version': '1.0'
        },
        'chemical_info': {},
        'assessment_info': {},
        'pdf_files': []
    }
    
    all_pdf_links = []
    
    try:
        # è®¿é—®ä¸»é¡µé¢
        driver.get(url)
        time.sleep(3)
        
        main_html = driver.page_source
        
        # æŸ¥æ‰¾æ‰€æœ‰iframe
        iframes = driver.find_elements(By.TAG_NAME, "iframe")
        
        # è®¿é—®æ¯ä¸ªiframeï¼ˆä¸ä¿å­˜HTMLï¼‰
        for i, iframe in enumerate(iframes, 1):
            try:
                iframe_src = iframe.get_attribute('src')
                
                if not iframe_src or 'about:blank' in iframe_src:
                    continue
                
                # åˆ‡æ¢åˆ°iframe
                driver.switch_to.default_content()
                driver.switch_to.frame(iframe)
                time.sleep(2)
                
                # è·å–å†…å®¹
                iframe_html = driver.page_source
                iframe_base_url = urljoin(url, iframe_src)
                
                # æŸ¥æ‰¾PDFé“¾æ¥
                pdfs = find_all_pdf_links(iframe_html, iframe_base_url)
                if pdfs:
                    all_pdf_links.extend(pdfs)
                
                # æå–æ•°æ®
                if 'SIDS.aspx' in iframe_src:
                    result['chemical_info'] = extract_chemical_data(iframe_html)
                elif 'SidsOrganigrame.aspx' in iframe_src:
                    result['assessment_info'] = extract_assessment_data(iframe_html)
                
                # åˆ‡æ¢å›ä¸»é¡µé¢
                driver.switch_to.default_content()
                
            except Exception as e:
                driver.switch_to.default_content()
                continue
        
        # ç›´æ¥è®¿é—®iframe URL
        soup = BeautifulSoup(main_html, 'html.parser')
        direct_urls = []
        for iframe_elem in soup.find_all('iframe'):
            src = iframe_elem.get('src')
            if src and 'about:blank' not in src:
                full_url = urljoin(url, src)
                direct_urls.append(full_url)
        
        for direct_url in direct_urls:
            try:
                driver.get(direct_url)
                time.sleep(2)
                
                direct_html = driver.page_source
                
                # æŸ¥æ‰¾PDF
                pdfs = find_all_pdf_links(direct_html, direct_url)
                if pdfs:
                    all_pdf_links.extend(pdfs)
                
                # æå–æ•°æ®
                if 'SIDS.aspx' in direct_url:
                    chem_data = extract_chemical_data(direct_html)
                    if chem_data:
                        result['chemical_info'].update(chem_data)
                elif 'SidsOrganigrame.aspx' in direct_url:
                    assess_data = extract_assessment_data(direct_html)
                    if assess_data:
                        result['assessment_info'].update(assess_data)
                
            except Exception as e:
                continue
        
        # å»é‡PDFé“¾æ¥
        seen = set()
        unique_pdfs = []
        for pdf in all_pdf_links:
            if pdf['url'] not in seen:
                seen.add(pdf['url'])
                unique_pdfs.append(pdf)
        
        all_pdf_links = unique_pdfs
        
        # ä¸‹è½½PDF
        for i, pdf in enumerate(all_pdf_links, 1):
            success, orig_name, md5_name, size, error = download_pdf_with_md5(
                pdf['url'], pdf_folder, session
            )
            
            pdf_info = {
                'index': i,
                'original_filename': pdf['filename'],
                'url': pdf['url'],
                'download_success': success
            }
            
            if success:
                pdf_info['filemd5'] = md5_name.replace('.pdf', '')  # åªä¿å­˜MD5å€¼
                pdf_info['saved_filename'] = md5_name
                pdf_info['file_size_bytes'] = size
                pdf_info['file_size_kb'] = round(size / 1024, 2)
            else:
                pdf_info['error'] = error
                pdf_info['filemd5'] = None
                pdf_info['saved_filename'] = None
            
            result['pdf_files'].append(pdf_info)
            
            time.sleep(1)
        
        return result
        
    except Exception as e:
        print(f"    âœ— å¤„ç†å¤±è´¥: {e}")
        return None


def process_excel_batch(excel_file, pdf_folder="oecdpdfs"):
    """
    æ‰¹é‡å¤„ç†Excelæ–‡ä»¶ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
    
    å‚æ•°:
        excel_file: Excelæ–‡ä»¶è·¯å¾„
        pdf_folder: PDFä¿å­˜æ–‡ä»¶å¤¹
    """
    if not DEPS_AVAILABLE:
        print("\nâŒ ç¼ºå°‘å¿…è¦çš„åº“")
        print("è¯·è¿è¡Œ: pip install selenium webdriver-manager beautifulsoup4 pandas openpyxl requests")
        return
    
    print("=" * 80)
    print("OECDæ‰¹é‡å¤„ç† - Excelç‰ˆæœ¬")
    print("=" * 80)
    print(f"Excelæ–‡ä»¶: {excel_file}")
    print(f"PDFæ–‡ä»¶å¤¹: {pdf_folder}\n")
    
    # åˆ›å»ºPDFæ–‡ä»¶å¤¹
    Path(pdf_folder).mkdir(parents=True, exist_ok=True)
    
    # è¯»å–Excel
    print("æ­¥éª¤1: è¯»å–Excelæ–‡ä»¶...")
    try:
        df = pd.read_excel(excel_file, engine='openpyxl')
        print(f"âœ“ æˆåŠŸè¯»å–ï¼Œå…± {len(df)} è¡Œæ•°æ®\n")
    except Exception as e:
        print(f"âœ— è¯»å–Excelå¤±è´¥: {e}")
        return
    
    # ç¡®ä¿Måˆ—å­˜åœ¨
    if 'M' not in df.columns:
        df['M'] = None
    
    # æ£€æŸ¥æ–­ç‚¹ç»­ä¼ 
    total_rows = len(df)
    processed_rows = df['M'].notna().sum()
    remaining_rows = total_rows - processed_rows
    
    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"  æ€»è¡Œæ•°: {total_rows}")
    print(f"  å·²å¤„ç†: {processed_rows}")
    print(f"  å¾…å¤„ç†: {remaining_rows}\n")
    
    if remaining_rows == 0:
        print("âœ“ æ‰€æœ‰æ•°æ®å·²å¤„ç†å®Œæˆï¼")
        return
    
    # åˆå§‹åŒ–Selenium
    print("æ­¥éª¤2: åˆå§‹åŒ–æµè§ˆå™¨...")
    try:
        service = Service(ChromeDriverManager().install())
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--window-size=1920,1080')
        
        driver = webdriver.Chrome(service=service, options=options)
        print("âœ“ æµè§ˆå™¨å·²å¯åŠ¨\n")
    except Exception as e:
        print(f"âœ— æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # åˆå§‹åŒ–requests session
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    })
    
    try:
        # å¤„ç†æ¯ä¸€è¡Œ
        print("æ­¥éª¤3: å¼€å§‹æ‰¹é‡å¤„ç†...\n")
        print("=" * 80)
        
        success_count = 0
        error_count = 0
        
        # ä»ç¬¬2è¡Œå¼€å§‹ï¼ˆç´¢å¼•1ï¼‰ï¼Œè·³è¿‡è¡¨å¤´
        for idx in range(1, len(df)):
            row = df.iloc[idx]
            
            # æ£€æŸ¥Måˆ—æ˜¯å¦å·²æœ‰æ•°æ®ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
            if pd.notna(row.get('M')):
                continue  # è·³è¿‡å·²å¤„ç†çš„è¡Œ
            
            # è·å–Kåˆ—çš„URLï¼ˆç´¢å¼•10ï¼Œå› ä¸ºKæ˜¯ç¬¬11åˆ—ï¼‰
            url = row.iloc[10] if len(row) > 10 else None
            
            if pd.isna(url) or not url:
                print(f"[è¡Œ {idx + 1}] è·³è¿‡ï¼šURLä¸ºç©º")
                continue
            
            print(f"\n[è¡Œ {idx + 1}/{total_rows}]")
            print(f"  URL: {url}")
            print(f"  å¤„ç†ä¸­...")
            
            # å¤„ç†URL
            result = process_single_url(url, pdf_folder, driver, session)
            
            if result:
                # è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                json_str = json.dumps(result, ensure_ascii=False)
                
                # å†™å…¥Måˆ—
                df.at[idx, 'M'] = json_str
                
                # ä¿å­˜Excelï¼ˆå¢é‡ä¿å­˜ï¼‰
                df.to_excel(excel_file, index=False, engine='openpyxl')
                
                # ç»Ÿè®¡ä¿¡æ¯
                pdf_count = len(result['pdf_files'])
                success_pdfs = sum(1 for p in result['pdf_files'] if p['download_success'])
                
                print(f"  âœ“ æˆåŠŸ!")
                print(f"     åŒ–å­¦å“: {result['chemical_info'].get('chemical_name', 'N/A')}")
                print(f"     PDF: {success_pdfs}/{pdf_count} ä¸ªä¸‹è½½æˆåŠŸ")
                
                if result['pdf_files']:
                    for pdf in result['pdf_files']:
                        if pdf['download_success']:
                            print(f"       - {pdf['original_filename']} â†’ {pdf['filemd5']}.pdf")
                
                success_count += 1
            else:
                print(f"  âœ— å¤±è´¥")
                error_count += 1
            
            print(f"  è¿›åº¦: {success_count + error_count}/{remaining_rows}")
            
            # æ·»åŠ å»¶è¿Ÿ
            time.sleep(2)
        
        print("\n" + "=" * 80)
        print("æ‰¹é‡å¤„ç†å®Œæˆï¼")
        print("=" * 80)
        print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"  æˆåŠŸ: {success_count}")
        print(f"  å¤±è´¥: {error_count}")
        print(f"  æ€»è®¡: {success_count + error_count}")
        print(f"\nğŸ“ PDFä¿å­˜åœ¨: {os.path.abspath(pdf_folder)}")
        print(f"ğŸ“„ Excelå·²æ›´æ–°: {os.path.abspath(excel_file)}")
        
        # ç»Ÿè®¡PDF
        pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]
        if pdf_files:
            total_size = sum(os.path.getsize(os.path.join(pdf_folder, f)) for f in pdf_files)
            print(f"\nğŸ“„ PDFæ–‡ä»¶ç»Ÿè®¡:")
            print(f"  æ–‡ä»¶æ•°é‡: {len(pdf_files)}")
            print(f"  æ€»å¤§å°: {total_size / (1024*1024):.2f} MB")
        
        print("\n" + "=" * 80)
        
    except KeyboardInterrupt:
        print("\n\nâš  ç”¨æˆ·ä¸­æ–­")
        print("å·²å¤„ç†çš„æ•°æ®å·²ä¿å­˜åˆ°Excelï¼Œå¯ä»¥ç¨åç»§ç»­è¿è¡Œï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰")
    except Exception as e:
        print(f"\n\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        driver.quit()
        print("\næµè§ˆå™¨å·²å…³é—­")


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    print("\n" + "=" * 80)
    print("OECDæ‰¹é‡å¤„ç†å·¥å…·")
    print("=" * 80)
    print("\nåŠŸèƒ½:")
    print("  âœ“ ä»Excelçš„Kåˆ—è¯»å–URL")
    print("  âœ“ æå–åŒ–å­¦å“å’Œè¯„ä¼°æ•°æ®")
    print("  âœ“ ä¸‹è½½PDFï¼ˆç”¨MD5å‘½åï¼‰")
    print("  âœ“ JSONç»“æœå†™å…¥Måˆ—")
    print("  âœ“ æ”¯æŒæ–­ç‚¹ç»­ä¼ ")
    print("\n" + "=" * 80 + "\n")
    
    # Excelæ–‡ä»¶è·¯å¾„
    excel_file = "oecdsids.xlsx"
    
    # å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        excel_file = sys.argv[1]
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(excel_file):
        print(f"âŒ Excelæ–‡ä»¶ä¸å­˜åœ¨: {excel_file}")
        print(f"\nè¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨ï¼Œæˆ–æŒ‡å®šæ­£ç¡®çš„æ–‡ä»¶è·¯å¾„:")
        print(f"  python batch_process_excel.py your_file.xlsx")
        return
    
    # å¼€å§‹å¤„ç†
    process_excel_batch(excel_file)


if __name__ == "__main__":
    main()